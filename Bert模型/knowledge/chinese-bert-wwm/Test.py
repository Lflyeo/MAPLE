import pandas as pd
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.metrics import precision_score, recall_score, f1_score

# ==================================
# ğŸ“‚ è¯»å– Excel æ–‡ä»¶
# ==================================
# âš ï¸ è¯·ä¿®æ”¹ä¸ºä½ çš„å®é™…æ–‡ä»¶è·¯å¾„
df = pd.read_excel("fold_2_best_model_test_results.xlsx", sheet_name="Predictions")

# ==================================
# ğŸ§© æ•°æ®é¢„å¤„ç†ï¼šå°†æ ‡ç­¾å­—ç¬¦ä¸²æ‹†åˆ†æˆåˆ—è¡¨
# ==================================
df["true_label"] = df["true_labels"].astype(str).apply(lambda x: [i.strip() for i in x.split(",")])
df["pred_label"] = df["pred_labels"].astype(str).apply(lambda x: [i.strip() for i in x.split(",")])

# ==================================
# ğŸ§® æ ‡ç­¾äºŒå€¼åŒ–ï¼ˆå¤šæ ‡ç­¾ä»»åŠ¡æ‰€éœ€ï¼‰
# ==================================
mlb = MultiLabelBinarizer()
y_true = mlb.fit_transform(df["true_label"])
y_pred = mlb.transform(df["pred_label"])

# ==================================
# âš™ï¸ å®šä¹‰è®¡ç®—å‡½æ•°
# ==================================
def calc_metrics(y_true, y_pred, average_type):
    return {
        "Precision": precision_score(y_true, y_pred, average=average_type, zero_division=0),
        "Recall": recall_score(y_true, y_pred, average=average_type, zero_division=0),
        "F1": f1_score(y_true, y_pred, average=average_type, zero_division=0)
    }

# ==================================
# ğŸ“ˆ è®¡ç®— micro / macro / weighted ä¸‰ç§å¹³å‡
# ==================================
results = {
    "micro": calc_metrics(y_true, y_pred, "micro"),
    "macro": calc_metrics(y_true, y_pred, "macro"),
    "weighted": calc_metrics(y_true, y_pred, "weighted")
}

# ==================================
# ğŸ–¨ï¸ æ‰“å°ç»“æœ
# ==================================
print("=== ğŸ“Š Evaluation Results ===")
for avg_type, metrics in results.items():
    print(f"\n[{avg_type.upper()} Average]")
    print(f"Precision: {metrics['Precision']:.4f}")
    print(f"Recall:    {metrics['Recall']:.4f}")
    print(f"F1 Score:  {metrics['F1']:.4f}")

# ==================================
# âœ… è¾“å‡ºæ¨¡å‹é¢„æµ‹ç»“æœå¯¹ç…§è¡¨
# ==================================
df["Pred_Correct"] = df.apply(lambda x: set(x["true_label"]) == set(x["pred_label"]), axis=1)
print("\n=== ğŸ§¾ Sample Predictions ===")
print(df.head(10)[["true_label", "pred_label", "Pred_Correct"]])
